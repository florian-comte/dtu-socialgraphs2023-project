introduction forecasting behavior large-scale real-world system directly first principle often requires solving highly-nonlinear governing equation high-dimensional ordinary differential equation ode partial differential equation pdes high-fidelity simulation dynamical system become intractable especially online control algorithm requires multiple forecast per second using low-powered embedded device situation like arises example smart heating ventilation air conditioning hvac system attempt optimize temperature distribution air room using partial measurement time writing paper system incapable real-time complex simulation already run low-dimensional pre-trained model invite development high-quality reduced order model rom therefore rom essential enabling design optimization uncertainty propagation predictive modeling control dynamical system order enable control high dimensional dynamical system rom training method need identify low-dimensional manifold along dynamic manifold together yield high-accuracy prediction long-term stability traditional rom projection-based e.g dynamic mode decomposition dmd proper orthogonal decomposition pod transform trajectory high-dimensional dynamical system suitable sense optimal low-dimensional subspace projection lead truncation higher order mode parametric uncertainty result large prediction error time due deterioration basis function spatial mode one challenge pod method intrusive nature i.e requiring access solver code overcome operator inference approach utilize svd-based model reduction exploit lifting fit latent space dynamic data polynomial typically quadratic model model however limited representation power quadratic e.g lift learn approach require custom-tailored svd-based optimization technique thrust overcome challenge significant effort invested developing autoencoder-based reduced-order model popular nonlinear rom technique yield accurate stable rom practice however autoencoder-based rom require datasets densely cover hypothetical infinite dimensional phase portrait dynamical system moreover large demand training data significantly limit use model physic application data expensive obtain another severe challenge utilizing rom come poor out-of-distribution performance especially fundamentally impossible practitioner obtain data cover entire distribution possible data input example hvac application one may collect data room two window room every possible number window atmospheric lidar application may conduct experiment certain terrain never conduct experiment sort terrain situation embedding knowledge physic model becomes necessary improve extrapolation performance several approach recently proposed instance seminal work tried determine underlying structure nonlinear dynamical system data using symbolic regression recently cranmer employed symbolic regression conjunction graph neural network gnn encouraging sparse latent representation extract explicit physical relation showed symbolic expression extracted gnn generalized out-of-distribution data better gnn however symbolic regression also suffers excessive computational cost may prone overfitting another example incorporating physic rom use parametric model latent space e.g using sparse identification nonlinear dynamic sindy instance used chain-rule based loss tie latent-space derivative observable-space derivative simultaneous training autoencoder latent dynamic however loss highly sensitive noise data especially evaluating time-derivatives finite difference required collocation-based enforcement physic i.e projection candidate function governing equation enforce chain rule instead finite difference could address numerical difficulty recently liu used autoencoder architecture koopman theory demonstrate combining autoencoders enforcing linear dynamic latent space may result interpretable rom however linearity may expressive enough complex dynamic multiple basin attraction finally recent work neuralode node show way fit arbitrary non-linear model e.g network latent space dynamic model significantly extending set model latent dynamic one train efficiently paper employ autoencoders perform nonlinear model reduction along node latent space model complex nonlinear dynamic choose neural ode latent space dynamic representation ability model highly non-linear dynamic especially important application limit size latent space dimension goal reduce demand training data improve overall forecasting stability challenging training condition end build idea classical collocation method numerical analysis embed knowledge known governing equation latent-space dynamic rom described method section experiment section show addition physics-informed loss allows exceptional data supply strategy improves performance rom data-scarce setting training high-quality data-driven model impossible demonstrate approach reduces need large training data-sets produce highly-accurate long-term stable model also lead discovery compact latent space especially important application compressed sensing control method reduced-order model non-linear latent dynamic consider autonomous dynamical system finite space ^n\ given aligned aligned real-world application often expensive solve directly high-dimensional however variety work provided theoretical empirical evidence many physical system evolve manifold ^m\ lower dimension space dynamic evolve according generally unknown function aligned aligned call space observable space latent space invertible mapping observable latent space known one predict dynamic system future time projecting initial condition latent space integrating dynamic latent space mapping resulting trajectory back observable space aligned aligned ^t\varvec aligned aligned refer triplet reduced-order model rom often case given system exists rom relation hold exactly case seek approximation rom minimizes difference data prediction chosen class model _\theta _\theta h_\theta parameterized multiple real-world application necessitate using rom instead integrating relation directly example integrating may computationally intractable especially platform limited computing capability embedded autonomous device instance hvac system solving mean solving navier–stokes equation fine grid real time exceeds computing capability current-generation appliance hand integrating may cheap finally even solving possible real time e.g utilizing remote cluster executing control resulting model end-goal hvac system may still intractable indeed executing control requires multiple evaluation iteration control even efficient algorithm known date figure illustration autoencoer structure neural ode latent space data-driven part loss function aim minimize sum two objective prediction loss reconstruction loss prediction loss minimizes difference data trajectory model prediction ensure temporal consistency latent space dynamic reconstruction loss ensures accurate reconstruction individual snapshot ensuring autoencoder behaves invertible mapping snapshot full size image architecture work model fully-connected neural network _\theta\ _\theta\ h_\theta\ respectively specifically pair modelled autoencoder _\theta _\theta modelled fully-connected network h_\theta\ figure visualizes architecture model data-driven loss similar prior work define data-driven loss data sum reconstruction prediction loss former ensures _\theta\ _\theta\ inverse mapping whereas latter match model prediction available data illustrated fig formally given set trajectory _i\ trajectory set snapshot correspond recorded state system time-steps t_j\ loss function data _\theta\ defined aligned data _\theta 2\sigma j=1 ^p\left\| x_i t_j _\theta _\theta x_i t_j ^2\right aligned aligned j=1 _\theta _\theta x_i t_1 t_1 t_j dt\right x_i t_j aligned standard deviation observation noise note trajectory _i\ may captured time-frame may use distinct possibly non-uniform step-size case loss function modified accordingly implementation affected evaluating integral part handled torchdiffeq library support non-uniform time-frames within batch simplify notation without loss generality rest paper assume trajectory recorded time-frame uniform step-size forecast behavior system latent space apply technique neural ordinary differential equation neural ode node utilizes adjoint sensitivity method back-propagate gradient integral neural ode demonstrated better ability model highly non-linear dynamic compared linear model dimensionality dynamic variable limited especially useful application size latent space dimension need small physics-informed loss recent work liu proposed method utilizing knowledge governing equation d\varvec finite-dimensional approximation koopman eigenfunctions linear latent dynamic extend approach non-linear regime note true mapping following hold aligned d\varvec d\varvec d\varvec d\varvec ^t\varvec aligned hand definition aligned d\varvec aligned combining get aligned ^t\varvec aligned figure physics-informed loss function compare gradient field current latent space correctly-learned field latent space set collocation point full size image equation link dynamic encoder known equation true hence shown fig knowledge assimilated model evaluating set carefully sampled point aligned physic _\theta h_\theta _\theta _\theta _\theta _\theta aligned refer point _i\ collocation point collocation point define collocation pair collocation point sample space im_ satisfy three condition ordered importance simplicity computationally cheap evaluate especially important pde system may involve high-order derivative representativeness _j\ cover space state one aim improve model performance stability collocation point model might encounter represented data snapshot best candidate feasibility word x_j\ attainable state system collocation point outside may downgrade performance autoencoder forcing invertible function domain outside thus optimal sampling procedure collocation point _j\ domain-specific designed given particular system available data _i\ show example condition implemented real system following section definition collocation point confused classic notion collocation point finding numerical solution differential equation classic notion refers set point time t_0 t_0 c_1h t_0 c_2h t_0 chosen obtain optimal local interpolant solution differential equation time-period t_0\ t_0 example collocation point runge-kutta method defined provide optimal gauss-legendre interpolant order coefficient c_1 c_s\ come respective butcher table contrast define collocation point pair example mapping definition built around solving inverse problem approximating f_\theta follows recent work develops upon definition ref difference sample space instead sampling spatiotemporal domain sample appropriate function space combined loss function train model optimizing sum physics-informed loss data-driven loss aligned _\theta physic _\theta data _\theta aligned data _\theta say model purely physics-informed similarly physic _\theta say model purely data-driven say model hybrid coefficient _i\ hyper-parameters need tuned using validation dataset however experiment paper set _i\ either balance physic _\theta\ data _\theta\ choice sample batch training data specifically set number collocation point per batch batch equal number trajectory per batch batch time number time-steps batch tk_ batch way physic _\theta\ data _\theta\ represent loss tk_ batch snapshot system providing average similar contribution information overall loss function laborious approach hyper-parameter tuning yield sufficient systematic advantage justify labour compared simple strategy use pytorch implementation adam algorithm optimization evaluate _\theta physic _\theta\ _\theta data _\theta\ use torchdiffeq pytorch -compatible implementation neural ode framework best knowledge first framework combine non-linear latent-dynamics neural ode autoencoders physics-informed loss term thus call framework physics-informed neural ode pinode figure use toy example—a lifted duffing oscillator—to show possible fill gap data collocation point specifically hybrid model able learn dynamic two additional basin attraction represented dataset shown top-rightmost frame without collocation point model doe infer dynamic unseen region correctly full size image experiment experiment section organized follows first illustrate idea behind framework study performance high-dimensional ode—a lifted duffing oscillator show non-linear latent dynamic overcomes limitation dmd koopman network handling multiple basin attraction within one model also show using physics-informed loss sufficient reconstructing behaviour basin attraction represented data finally demonstrate purely data-driven model may highly-accurate short-term highly unstable long-term even data abundant show physics-informed approach improves long-term stability model multiple order magnitude next study framework performance burger equation show non-linear latent dynamic model yield compact latent space representation linear counterpart accuracy compact latent space representation allow stable long-term prediction iii presence significant noise data use collocation point improves stability providing extra source information noise-free certain scenario training collocation point yield better model training data even vast amount data available last observation show contribution physics-informed loss may surpass data-based loss especially data severely limited noisy lifted duffing oscillator duffing oscillator dynamical system d\varvec aligned aligned dz_1 z_2 dz_2 z_1 z_1^3 aligned aligned figure non-linearity latent dynamic autoencoder employed teh pinode hybrid model important accurate long-term extrapolation dmd model pikn hybrid model unable extrapolate dynamic collocation point full size image phase portrait randomly sampled trajectory system visualized fig left frame depending total energy trajectory always stay one three region left lobe right lobe outer area visualized red green blue respectively create synthetic high-dimensional system retains property lift duffing trajectory higher-dimensional space applying invertible transformation aligned a\varvec i.i.d aligned hence system ^2\ span treat observable space dynamical system obeys following aligned d\varvec a^ta a^t\varvec 1/3 ^t\varvec a^ta a^t\varvec 1/3 aligned thus created high-dimensional dynamical system multiple basin attraction dynamic known experiment generate trajectory _i\ 0.1\ taken left lobe region red also sample 50,000 collocation point _j\ right green outer blue region sampling u\left -3/2 3/2 applying transformation example condition collocation point discussed method section trivially satisfied train two pinode model data-driven model trajectory hybrid model trajectory collocation point model share architecture training parameter detailed supplementary appendix a.1 training invert mapping project model high-dimensional prediction unseen initial condition onto true low-dimensional manifold visualized fig make two observation result displayed fig first purely data-driven model unable extrapolate outside training region using data region observation consistent conclusion related work neural network interpolate well struggle extrapolation task second see collocation point provided enough extra information model predict nearly perfectly region trajectory provided observation suggests one use collocation point cover gap data improve extrapolation accuracy model figure box plot prediction error three pinode model data-driven physics-informed hybrid time measured multiple training time period i.e x=3t\ refers time-range two three training time-periods away full size image ability neural ode model nonlinear dynamic latent space demonstrated fig figure show comparison hybrid pinode model hybrid pikn model dmd trained using dataset pikn differs pinode linear latent dynamic lz\ finite-dimensional approximation koopman operator instead general non-linear dynamic operator h_\theta pikn set time expansion dimension true manifold observe fig pikn unable extrapolate dynamic unseen area correctly using collocation point eventually trajectory collapse onto attractor also seen dmd show even worse performance could attributed linear model reduction next experiment show collocation point stabilize long-term prediction model even data part space available illustrate generate dataset trajectory trajectory per red green blue area 50,000 collocation point uniformly distributed among three lobe train three model data-driven physics-informed hybrid version pinode relative performance three model evaluated fig x-axis represents test time-horizon multiple training trajectory length -axis show box plot prediction mean squared error mse corresponding unseen trajectory within specific period example 2t\ represents time-period -axis show distribution prediction error within period figure show performance data-driven model degrades quickly forecasting time-period increase despite excellent performance forecasting within training time-period physics-informed model start modest performance training time horizon maintains stable performance forecasting far ahead hybrid model turn combine near-term accuracy long-term stability yielding best result time period burger equation study performance framework burger equation -\pi -periodic boundary condition aligned u_t uu_x -\pi aligned u_t\ u_x\ represent partial derivative time first second spatial derivative respectively burger equation pde occurring application acoustic gas fluid dynamic traffic flow significantly smaller one system exhibit strong non-linear behaviour called advection-dominated otherwise large system called diffusion-dominated case former linear projection method pod become inaccurate true solution space slow decaying kolmogorov n-width manifesting slow decaying singular value therefore section focus advection-dominated burger equation set 0.01\ generate trajectory discretize spatial domain -\pi grid-points solve 0.1\ using spectral solver generate diverse set initial condition sum first harmonic term random coefficient aligned a_k\cos b_k\sin k+1 a_k b_k aligned generate collocation point use family function used initial condition additionally randomize presence individual frequency sum aligned p_ka_k\cos q_kb_k\sin k+1 a_k b_k p_k q_k 1/2 aligned choose family collocation point meet condition 2.5 first family representative state space im_f region interest moving wave-fronts second smooth set function doe contain unattainable state finally importantly value u_x\ consequently u_t\ computed analytically make especially cheap sample large number collocation point compressibility latent space lifted duffing oscillator section showed non-linear finite-dimensional latent dynamic model necessary building compact rom high-dimensional lifted duffing system necessarily case burger equation since exists cole-hopf transformation linearizes dynamic burger equation however latent-space non-linearity principle utilized finding compact latent space representation increasing forecast accuracy fixed latent space dimension section demonstrate pinode achieve goal figure pinode hybrid model utilized latent space dimension time efficiently term mse pikn hybrid model modelling low-viscosity highly-nonlinear burger equation left frame difference performance grows forecasting two time farther training period central frame pikn suffers long-term instability due presence eigenvalue positive real part latent dynamic matrix right frame frame plot eigenvalue latent-space matrix pikn model frame legend right frame refers dimension latent space used corresponding pikn model full size image experiment generate 16,384 trajectory described also generate 100,000 collocation point described purpose using large amount data allow trained model achieve best performance specified latent space dimension evaluate performance model test data two different time-frames training data interpolation two time longer training data extrapolation detail experimental setup provided supplementary appendix a.4 fig compare performance three model dmd pikn hybrid pinode hybrid first notice dmd doe perform well test data despite achieving training loss 10^ observation consistent earlier work illustrates well combination linear encoder linear latent dynamic operator may sufficient modelling highly-nonlinear phenomenon second notice pinode achieves better performance given latent space dimension compared pikn instance 16\ fig left pane pinode achieves time lower mean squared error pikn achieves performance 512\ importantly pinode maintains low prediction error longer-term horizon extrapolation time case pikn fig center pane consequence latent-dynamics matrix lz\ pikn eigenvalue positive real part implies long-term instability fig right pane although progress literature research needed understand enforce stability constraint pikn one doe need enforcement pinode exhibit stable behaviour training low-data regime collocation point next experiment study relative efficiency using collocation point using data low-data regime frequently case small number simulation measurement obtained physical system interest due computational time budget constraint would like compensate lack sufficient data providing collocation point considerably cheaper generate section show chosen appropriately collocation point effectively used training model low-data regime contribution model accuracy may even surpass contribution data figure example harmonic bell-curve bump initial condition well resulting solution column respectively full size image figure comparison achievable mse relative full data regime trajectory data scarce collocations-based physics-informed loss improves forecasting accuracy rom average time lower mse compared data-only regime shown experiment burger equation type initial condition harmonic bell-curve used physics-only model top-right corner right frame outperformed data-rich model experiment bottom-left corner full size image illustrate trade-off data collocation train one model using varying combination number trajectory collocation point training datasets gauge extrapolation power model use trajectory three type initial condition harmonic bell-curve bump see fig illustration generate trajectory bump initial condition training data use harmonic family initial condition described generating training collocation use two test datasets trajectory bump ass within-distributuion performance left frame mix trajectory bump bell-curve harmonic initial condition trajectory ass out-of-distribution performance test data trajectory two time longer training trajectory detail experimental setup provided supplementary appendix a.5 figure present reconstruction mse test datasets obtained pinode model trained varying combination trajectory collocation point percentage mse achievable pinode model trained full trajectory alone collocation pinode model use latent space dimension m=16\ figure demonstrates adding collocation point consistently improves model performance experiment moreover sufficient number collocation point added training model fewer training trajectory always able outperform model trained available trajectory collocation average collocation-aided model time better within-distribution out-of-distribution reconstruction relative purely data-driven version model addition noticed model used collocation point perform better data-rich model especially predicting dynamic unseen initial condition fig right pane top-right bottom-left corner figure first subplot show relative error solving burger equation test unseen initial condition two model pinode hybrid pinode data-driven model interpolate well purely data-driven model fails extrapolate past training time-horizon left red vertical line pinode-hybrid provides stable long-term prediction point ability correctly discover low-dimensional manifold dynamic full size image also notice hybrid model yield stable accurate prediction relative purely data-driven counterpart forecasting far beyond training time-period fig visualize prediction test two model data-driven model bottom-left corner fig hybrid model bottom-right corner fig red line separate time-period training time-period forecasting hybrid model error stay 10^ even forecasting time farther trained contrast data-driven model show low error within training time-region forecast error grow quickly forecasting beyond figure collocation point improve result three model fix model inherent shortcoming like instability linear latent dynamic full size image finally observe using collocation point benefit model like dmd pikn illustrate replicate experiment fig number trajectory bump pinode pikn dmd figure show root mean squared error rmse test data prediction function number collocation point used training figure illustrates prediction error increasing prediction horizon going left right demonstrates case pinode benefit available collocation point leftmost panel show every model improves one-step-ahead prediction dmd quickly achieving near-optimal performance however forecast horizon increased timesteps ahead length training trajectory dmd failed correctly forecast long-term trajectory removed figure improve legibility pikn model improved one-step-ahead 1st pane interpolation performance 2nd pane factor also improved extrapolation performance 40-steps prediction 3rd pane failed extrapolate step 4th pane removed legibility attribute behavior pikn possibility latent dynamic operator pikn contains positive eigenvalue despite use collocation point robustness noise low-data regime section show use collocation point improves rom robustness noise data providing alternative noise-free source information figure physics-informed loss work safeguard prevents unbounded performance drop quality data degrades due noise namely solution hybrid loss converges solution physics-informed loss data-driven loss becomes uninformative performance purely data-driven method data-driven dmd grows unbounded since model alternative noise-independent source information full size image experiment use burger equation dataset containing trajectory bump initial condition 65,536 harmonic collocation point defined add i.i.d gaussian noise trajectory variance ranging 10^ 10\ reference data value lie noise level dominates data train four model pinode hybrid pinode data-driven pinode physics-informed dmd measure model out-of-distribution prediction error use test dataset bump gaussian harmonic initial condition described previous subsection prediction error displayed fig left pane prediction error purely physics-informed model red flat collocation point noise-free figure show high noise setting error purely data-driven model dmd pinode data-driven grows unbounded whereas performance hybrid model converges performance physics-informed model noise level increase hypothesise behavior due second part data combined loss turn noise derivative also turn noise aligned physic informative data noise aligned thus one think optimizing hybrid model training physics-informed model using noisy gradient descent fixed-variance noise optimization literature know certain condition sgd converges neighbourhood local minimum loss case physic high probability instead diverging hybrid model turn physics-informed model latter work performance safeguard high-noise regime right hand-side fig show example prediction performance model described data-driven hybrid model yield visually similar solution 10^ however former provides inadequate performance data dominated noise whereas hybrid model regime produce solution visually similar one physics-informed model produce rigorous analysis phenomenon seems possible lie outside scope paper discussion conclusion work demonstrated collocation point-based technique improve performance emerging class continuous-time physics-informed neural-network based reduced-order model first demonstrated incorporation collocation point training data cover gap training trajectory inform model underrepresented basin attraction approach alleviates demand large volume data common network-based model crucial application data scarce expensive second physics-informed loss may work safeguard providing noise-free source underlying dynamic third collocation point stabilize model long-term prediction allowing accurate forecasting far beyond training time horizon finally together using node-based non-linear latent dynamic adding physics-informed loss lead discovery compact latent space representation also yield accurate model simultaneous stability compactness especially important one aim use model together compressive sensing control algorithm respect computational complexity note adding collocation point training imposes computational burden adding data trajectory collocation point require computing integral forward time case data trajectory one clear limitation current work choice efficient collocation family design decision practitioner make author believe decision automated adopting existing approach classic work numerical approximation pdes leave future research another automation prompt future research deriving efficient way sampling collocation point possibly via applying modern adaptive learning technique finally although robustness noise low-data regime section provides rationale one may expect robustness hybrid model noise author believe rigorous analysis possible particularly one provides condition robustness guaranteed