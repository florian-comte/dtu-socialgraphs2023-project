introduction benefit cochlear implantation shown various study review safe cochlear implantation base essential realistic pre- intra-operative anatomical information cochlea adjacent inner ear structure select correct electrode obtaining appropriate cochlear coverage also prevent damage intracochlear structure furthermore anatomical information inner ear may used elucidate pathophysiology hearing loss balance disease large scale evaluation radiological data non-invasive radiological imaging temporal bone via computed tomography magnetic resonance imaging mri currently default strategy provide required anatomical information researcher clinician inside current routine workflow data inspected visually series multiplanar view mentally comprehend structure gather desired information automated creation advanced visualization basis semantic segmentation conjunction parameter estimation thus viewed highly beneficial process cochlear implantation particular inner ear clinical research general however generation accurate individualized analysis radiological data remains time cost labour intensive generation process bottlenecked step manual labeling key structure due complexity subtlety involved anatomy suffers inter- intra-observer variability realize envisioned benefit clinical purpose fix pitfall manual workflow two related task semantic segmentation inner ear structure localization landmark solved fully automated pipeline minimal user interaction success two fundamental task may act gateway fully automated cochlear duct length measurement supplanting current formula-based method relying bézier curve fitted manually set keypoints approach automatization goal consider recent progress field computer vision spurred disruptively deep learning convolutional neural network architecture choice achieving state-of-the-art performance image classification medical image analysis task alike drastically simplify via automation learning pre-assembled set training example careful compilation training example enables minimization intra- inter-observer variability contrast traditional machine learning algorithm deep neural net regarded adequate operating high-dimensional unstructured data like volume data due capacity internally construct salient data feature optimization phase integrated performant data-efficient deep learning framework automated analysis inner ear structure could beneficial fast preoperative planning large-scale clinical research related work starting point related literature traditional method like atlas-based framework cochlear structure segmentation inner ear fluid space iterative random-walk shape prior full inner ear segmentation data suggested past within realm deep learning based method exist several closely related recent study directly share goal automated analysis inner ear structure volumetric data following provide summary adjacent work heutink sequentially staged resnet unet used segment cochlea ultra-high resolution clinical data context automated cochlea length volume measurement proposed system formulated volume scan gap bridged using patch sampling approach iteratively integrated volume protocol leveraged primary resnet stage generate soft-masking probability map three different scale differing network soft mask utilized constrain output secondary unet stage previously identified foreground region augmentation protocol patch comprised in-plane rotation amplitude vertical flip author describe validation performed fixed set eight volume testing fixed set volume cross validation strategy described another adjacent study neve employed set cnn variant perform multi-class segmentation clinical volume temporal bone scan three backbone resnet unet newly proposed ahnet compared framework presented integrated pipeline 3dslicer software make use largest region extraction growing/shrinking algorithm pre-set intensity value postprocess convolutional neural network output augmentation protocol encompassed intensity oscillation intensity magnitude flipping first axis validation testing procedure work encompassed five-fold separate testing set volume work hussain leveraged open-source micro data cadaveric specimen gerber formulate multi-aspect cnn-based framework inner ear segmentation author introduced autocasnet three-stage cascade network specialized iterative segmentation coronal axial sagittal aspect data respectively author operated compared resnet unet seunet squeeze-and-excitation network backbone design choice tested validation testing done via 4-fold without independent test set formulation incorporated largest region extraction postprocess network prediction data augmentation described formulation fauser presented shape-regularized preoperative pipeline segment multiple anatomical risk structure volumetric temporal bone scan facilitate subsequent trajectory planning first study triplet multi-aspect unets similar approach hussain al. used segment structure contrast hussain al. adopted autocasnet sequential cascade convolutional neural network made use parallel inference axial sagittal coronal aspect-wise unet instance combined result using majority-voting strategy follow-up work replaced parallelized multi-aspect set unets two-staged unet implementation primary stage extracted volume interest secondary stage unet identical architecture produced segmentation anatomy described system shared key feature exploited registration probabilistic active shape model pasm separate downstream postprocessing step network prediction formulated using data clinical volume scan without data augmentation validation procedure consisted two-fold cross validation strategy without independent test set summarizing table related work concerned deep learning based automated analysis temporal bone scan provided table addition presented work operating imaging modality study volumetric magnetic resonance imaging mri data conducted recently refer two study engaged inner ear segmentation based data generated t2-weighted sequence notably vaidyanathan described pipeline based modified unet achieving inner ear segmentation dice score 0.86 mean standard deviation validation dataset 0.87 test dataset ahmadi employed v-net based pipeline conjunction atlas-based pre-localization pre-segmentation step achieve dice score 0.90 test dataset presented study concerned segmentation task mono- multi-class setting work however proposed segmentation landmark detection integrated multi-task problem since single bi-headed network performs segmentation landmark detection using multi-task learning approach multi-task learning produce improved result compared learning task separately applied successfully large variety task medical imaging including brain image analysis retinal image analysis inner ear analysis multi-task learning used reducing metal artifact post-implantation image contextualize presented work multi-task joint segmentation landmark detection look craniomaxillofacial cmf application since inner ear analysis otological application multi-task learning applied extensively example liu presented joint segmentation landmark detection framework cmf preoperative planning automated task comprised segmentation midface mandible bone structure localization bone teeth face landmark design reinterpreted landmark localization multi-class segmentation casting landmark individual class spirit semantic segmentation proposed framework leveraged layered setup quadruplet specialized unets operating different image resolution scale primary stage provided coarse segmentation coarse-global localization bony facial landmark secondary stage comprised two refinement model handled bone segmentation landmark detection separately secondary stage produced refined segmentation localization result using specialized bone tooth model purpose-cropped volume interest input primary stage zhang proposed context-guided unet based framework termed jsdnet joint midface mandible bone segmentation localization landmark cmf application architecture consisted cascade two sub-networks subsequently termed fc1 fc2 first network trained estimate displacement map landmark position second network conducted joint prediction bone segmentation map landmark heatmaps using scan data concatenated displacement map data input aim study main purpose project development validation fully automated pipeline jointly generate segmentation map inner ear structure localization landmark temporal bone scan framing problem multi-task learning problem implemented end-to-end trained solution algorithm based u-net architecture work provide contribution following aspect describing performant end-to-end differentiable u-net pipeline evaluated range in-house open source data clinical specimen origin directly comparing formulation single-task unets comparing formulation recently published state-of-the-art method temporal bone analysis application reimplementation joint segmentation localization architecture craniomaxillofacial application space providing series ablation study elucidating performance impact various design choice releasing labeled training validation dataset existence labeled data vital principally data-scarce restricted field medical imaging making open-source release labeled in-house datasets zenodo platform doi http last contribution element table overview comparison recently published state-of-the-art method machine-learning based inner ear segmentation full size table method datasets description design training evaluation performed basis two in-house datasets scanned siemens axiom-artis dynact fpvct scanner siemens healthcare gmbh erlangen germany using commercially available software syngo dynact siemens following acquisition parameter utilized dct head protocol tube voltage tube current pulse length 3.5 rotation angle frame angulation step 0.5 per frame slice thickness postprocessing step acquired data reconstructed kernel type sharp image characteristic exported digital imaging communication medicine dicom file format finally acquisition postprocessing protocol yield cuboidal voxels isotropic edge length 98.6 first in-house dataset ex-vivo comprised scan cadaveric temporal bone specimen split two subset training 38\ validation purpose second in-house dataset in-vivo comprised additional temporal bone scan recorded in-vivo pre-operative patient clinical practice data clinical dataset processed entirely anonymized retrospective fashion retrospective anonymized study conducted concordance local guideline principle declaration helsinki good clinical practice approved ethic committee university würzburg requirement informed consent study subject waived ethic committee university würzburg due anonymous data analysis article paragraph 4.1 bavarian hospital law baykrg permit use patient data within hospital research purpose research interest hospital without patient consent necessary instance dataset exclusively used hold-out set main gauge performance methodology total dataset element single instance showed otosclerotic lesion evaluated separately fact communicated element notation dataset described datasets inner ear structure manually annotated supervision cochlear implant surgeon k.r extensive experience cochlear analysis segmentation landmark position annotation performed 3dslicer software open-source software tool medical image analysis segmentation process comprised manual labeling via brush-based painting assisted mask generated via voxel intensity thresholding foreground label thus correspond inner ear labyrinth fluid space bounded bone membrane round oval window anatomical landmark label position helicotrema oval window round window marked voxel coordinate vector dataset released open-source zenodo repository doi 10.5281/zenodo.8277159 part work order examine performance generalization ability proposed methodology evaluation three publicly available open source datasets conducted datasets presented challenge machine learning model due significant difference imaging setup native voxel size annotation source open-source datasets provided similar inner-ear label segmentation purpose missing landmark position information two open source datasets supplemented author using 3dslicer based voxel coordinate based workflow used landmark annotation in-house datasets open access dataset wimmer consists n=23 specimen imaged clinical multi-slice msct device yielding voxel size specimen preserved embalming formalin thiel yielding two distinct subgroup respectively formalin subgroup embalming process altered contrast usually fluid-filled volume comparison anatomy see fig openear dataset sieber contains n=7 temporal bone scan cone-beam cbct modality isotropic voxel size preservation method specimen entailed multiple preservation step successive immersion five different fixation solution physical access canal drilling superior semi-circular canal recorded specimen embedded epoxy resin final step procedure also introduced contrast-shifts due modification fluid-filled volume well structural modification bone anatomy high-resolution dataset gerber provides n=17\ temporal bone scan using microct modality isotropic voxel size 16.3 19.3 dataset also two distinct subgroup larger smaller field-of-view respect inner ear anatomy contrast distribution two subgroup well-differentiable visible fig dataset generally contains dry specimen provide contrast in-vivo fluid-filled volume structure preparation method also erased round oval window membrane see fig panel imaging reconstruction procedure open-source datasets appears deviated standard operating procedure clinical imaging implies orientation open-source datasets volume differs predominant orientation visible in-house datasets visualized phenomenon accompanying binder notebook online resource http presented deep learning algorithm manually produced segmentation label considered basal ground truth segmentation label provided integer per-voxel label signify membership voxel salient anatomical structure ground truth landmark label given manually determined voxel coordinate vector summarizing information feature different datasets displayed table visual comparison various datasets comprising preparation-induced contrast difference imaging induced voxel size difference provided supplementary material sect s.4 table overview defining dataset parameter utilized development evaluation proposed pipeline full size table data processing augmentation figure visual rendition contrast distribution utilized datasets processing open-source datasets performed four identifiable cluster separately shaded blue region demarcates five standard deviation region average foreground voxel intensity training dataset dashed line mark corresponding average foreground voxel intensity upper right panel show exemplary slice training dataset lower left panel show slice embedded specimen dataset lower right panel show slice instance microct dataset full size image convolutional u-net architecture unaware pattern scale anisotropy induced varying voxel edge length thus datasets globally resampled isotropic voxel size a_v training dataset prior inference in-house fpvct data first clipped 0.01\ -th 0.99\ -th quantile value subsequently rescaled voxel intensity value mean unit standard deviation normal distribution emerges full datasets parameter value quantile mean standard deviation computed whole datasets respectively open-source datasets integrated histogram contrast matching scheme addition clipping rescaling computation key aspect correct fluid-space-wise contrast modification introduced various preservation scheme datasets formalin embalming immersion fixation solution resin embedding drying dataset frozen specimen exhibited well-preserved contrast normally fluid-filled cavernous structure membrane region like oval round window compared clinical dataset another open-source dataset processing element application clipping normalization procedure separately cluster visible fig process yielded remapping intensity distribution range training data subsequently diminishing input data domain shift algorithmically training inference volume instance divided sub-volumes termed chunk dimension individual chunk loaded processed separately network chunk-based processing strategy attenuates large memory consumption volumetric data convolution operation chunk generated sliding-window fashion isotropic stride voxels direction enhance effective training data amount variability traditional form training time augmentation performed stochastically applying variety augmenting transformation chunk used spatially intensity-wise transformation protocol intensity-wise augmentation randomly adjusted image contrast multiplication scaling factor application additive gaussian poisson noise lastly free angle rotation data randomly chosen spatial axis performed ablation study employed two different variation protocol augmentation protocol aug _\alpha\ fixed rotation first volume axis well free angle rotation amplitude employed second augmentation protocol encompassed fixed rotation three spatial free angle rotation larger amplitude information visualization concerning augmenting transformation provided supplementary material besides training data augmentation applied test-time data augmentation inference run external open-source datasets mitigate domain shift induced non-standard rotation state automated rotation input chunk angle three spatial resulting tta 10\ prediction averaged voxel level produce final prediction detail data processing training testing augmentation stated supplementary information sect s.4 algorithm network architecture inherently volumetric fpvct data u-net architecture shown appropriate architecture automated medical image analysis outperforming many alternative technique thusly fully convolutional deeply supervised u-net model assembled solve joint segmentation localization problem consisting common encoder block two task-specific decoder block skip inter-connections contracting encoder path hierarchically extract high level information decreasing spatial resolution strided convolution dual task segmentation landmark localization two separate decoder head envisioned draw upon contextualized high level feature extracted common encoder backbone expanding decoder path successively re-up-samples feature map utilizes extracted feature information supplemented localized detail via skip connection segment voxels semantic class localize landmark position via heatmap regression respectively visual overview utilized u-net architecture provided fig building upon heuristic derived nnunet default network architecture four level double convolution module basic building block consisting normalization layer convolution nonlinear activation function spatially contracting encoder path downsampling operation performed strided convolution kernel size isotropic stride feature map size channel count cascade given sequence higher lower encoder level number feature map held constant level transition encoder decoder pathway upsampling operation spatially expanding decoder path executed transposed convolution kernel size isotropic stride symmetric design enabled direct concatenation feature map produced left-adjacent encoder facilitating correct global localization salient structure segmentation localization described encoder cascade yielded tensor spatial dimension 64^3 32^3 16^3 8^3 every downsampling step figure overview block diagram encoder-decoder u-net architecture deep supervision terminal structure implemented joint segmentation landmark detection task u-net architecture entail single common encoder backbone two task-specialized decoder head cuboid symbolize feature map produced convolution operation number feature map stated every convolution block operation next cuboid single encoder backbone hierarchically distills contextualized information input data two specialized decoder head process distilled feature map solve semantic segmentation heatmap regression task addition u-net standard output terminal top-most decoder t_0\ deep supervision strategy adopted producing secondary output map lower decoder level terminal t_1\ t_3\ attention gating skip connection used filter feature map relevant foreground volume improve overall performance full size image segmentation head performed voxel-wise classification problem estimating pseudo-probability map n_x n_y n_z posse identical shape respect input final segmentation map obtained applying sigmoid nonlinearity last convolutional layer output architecture sketch fig segmentation head depicted light-blue colored structure heatmap head second specialized decoder structure attached common encoder backbone performed landmark localization task via intermediate step heatmap regression n_x n_y n_z heatmap head structurally similar segmentation head possessed three output channel facilitate localization landmark helicotrema oval round window furthermore heatmap head process last convolutional layer output sigmoid activation function coordinate prediction specific anatomical landmark subsequently computed corresponding heatmap via localization mode argmax operation ^3\ predicted coordinate vector denotes index landmark i.e apex cochlea oval window round window heatmap head depicted architecture diagram fig green colored structure decoder head attached common encoder backbone received feature map lowest level encoder structure successively upsampled using set internal weight supplied skip connection input internal u-net parameter learned minimizing composite loss function given consisting segmentation-specific term segmentation landmark-related heatmap-specific term heatmap regularization term segmentation loss computed linear combination cross-entropy loss dice loss sdsc dice loss function derived dice-soerensen similarity measure aligned segmentation heatmap sdsc mse aligned equation abbreviation denotes cross entropy sdsc denotes soft dice loss mse stand mean squared error mathematical definition utilized component loss function described detail supplementary material section s.6 mean squared error loss measured ground truth heatmap centered around landmark coordinate predicted heatmap ground truth heatmap corresponding landmark coordinate vector manual annotation thereby produced computing gaussian function voxel grid amplitude spread smoothly guide network learn accurate heatmaps voxel space utilized heatmap scheduling protocol protocol increased amplitude parameter decreased spread parameter according preset sequence value fixed training iteration number loss component segmentation heatmap also utilized construction auxiliary companion loss function deep supervision terminal final regularization term denotes neural network regularization imposing zero-mean gaussian prior weight yield well-known weight decay functional ^2\ embedded adamw optimizer weight decay strength hyperparameter performed multitude ablation experiment elucidate performance impact various design choice end defined base architecture using feature map cascade shown fig together instance norm leaky relu nonlinearity slope parameter 0.025 heatmap scheduling combined dice-cross-entropy-loss deep supervision starting fixed basal configuration mono-parametrically changed following aspect normalization layer loss function encoder depth channel dimension size augmentation strategy activation function attention gating global head-wise normalization layer tested batch norm loss function tested segmentation part pure dice loss squared dice loss logcoshdiceloss pure binary cross entropy loss disabled deep supervision training data augmentation scheme tested enhanced scheme aug _\beta\ augmentation activation function tested relu prelu gelu silu mish detail basal ablation architecture provided supplementary material sect s.1 attention gating tested partial head-wise global application attention gating every configuration trained three-fold tested hold set provide concrete baseline result published framework turned towards jsdnet described zhang originally described craniomaxillofacial application two-class bone segmentation tooth bone landmark detection since approach closely related reimplemented jsdnet architecture scratch closely possible applied described temporal bone analysis task provide explicit comparison data point deduced unavailable information architecture design training protocol cursory hyperparameter search original formulation also contain augmentation protocol thus performed two training experiment using described augmentation scheme aug one instance augmentation instance detailed description protocol jsdnet implementation provided supplementary material sect s.8 training implemented described modular dual headed deep neural network architecture pytorch three-fold cross validation strategy used dataset train validate network experiment fold randomly selected 38\ datasets training datasets validation dual-head 3d-u-net architecture trained fixed pre-set 7.5\times 10^ iteration starting fully random initialization initialization performed using uniform distribution gradient accumulation performed iteration experiment trapezoidal learning rate scheduling warmup plateau annealing phase used one iteration defined forward pas batch volume chunk loss computation computation corresponding gradient respect network parameter every iteration current model performance measured computing evaluation metric iou dsc segmentation head output mse mean absolute error mae heatmap head output well validation loss using validation datasets model parameter validation iteration yielding optimal evaluation criterion value last iteration saved result every training run employed three-fold cross validation strategy model parameter optimal evaluation criterion value used subsequent performance quantification validation test dataset catalogue ablation study defined basal training configuration similar baseunet architecture configuration outlined encompassed adamw optimizer weight decay parameter 0.025\ fixed heatmap schedule refer baseunet architecture training strictly adhered specified parameter mono-parametrically introduced variation employed glean respective performance impact optimizer tested stochastic gradient descent sgd adamw setting tested different weight decay parameter running average coefficient heatmap schedule tested three different amplitude spread sequence different iteration milestone training process gradient accumulation tested variable number accumulation iteration accum 1,5,10,15,20\ performance evaluation used dataset instance primary test model performance conclusion training phase since separate hold-out set experiment also show generalization skill model towards data sourced clinical practice processed open-source datasets used determine model performance data differing imaging setup testing result produced three-fold cross validation yielding mean standard deviation performance fold prediction joint segmentation detection algorithm evaluated computing variety evaluation criterion employed spatial overlap based performance metric dice-sørensen similarity score dsc primary metric intersection-over-union iou alternatively jaccard index volumetric similarity score evaluation segmentation prediction addition overlap-based metric utilized hausdorff distance contour- spatial distance based evaluation criterion take spatial structure boundary delineation segmentation account addition default hausdorff distance susceptible outlier calculated average hausdorff distance obtained supplanting maximum operation average voxels ablation experiment computed due substantial computational complexity evaluation localization task initialized computing predicted landmark coordinate application argmax operator output heatmaps yielding voxel position vector heatmap mode per-landmark euclidean distance ground truth voxel coordinate vector argmax coordinate vector computed since volume resampled isotropic voxel edge length prior insertion pipeline report unit-bearing parameter euclidean distance hausdorff distance hereafter voxel unit mathematical definition metric used work provided supplementary material sect s.6 result u-net trained using training protocol outlined random initialization preset number iteration completed training loss generally exhibited stable reduction low value validation performance reached high consistent plateau value see supplementary material sect s.3 detail observation indicated benign convergence behaviour algorithm confirmed successful avoidance overfitting first turn comparison single-task version baseunet architecture multi-task counterpart reimplementation jsdnet zhang result overall segmentation localization performance shown fig figure comparison segmentation left average localization performance right basal unet configuration trained respective single task blue multitask setting orange two manifestation jsdnet architecture zhang depicted one trained default augmentation scheme aug _\alpha\ one without augmentation original formulation error bar show standard deviation computed three-fold strategy full size image segmentation wise model displayed excellent performance validation dataset dice score 0.95 jsdnets 0.96 single- multi-task baseunet test dataset baseunet variant exhibited similar segmentation performance dice score 0.93 jsdnet variant suffered steeper performance drop-off augmentation-free trained variant offering slightly worse segmentation score 0.88 average performance hold-out datasets dice score 0.90 multi-task baseunet slightly differed negatively single-task 0.91 jsdnet showed larger dice score drop 0.76 0.75 augmented unaugmented training respectively localization task euclidean distance deviation gap single-task 5.6 voxel unit multi-task 4.8 voxel unit visible already validation data superior performance multi-task solution continued test dataset 5.6 voxel unit test dataset average 7.2 voxel unit loss localization precision larger single-task baseunet 6.3 voxel unit 10.0 voxel unit jsdnet trained augmentation comparably larger localization deviation 7.3 9.4 24.0 voxel unit validation dataset test dataset average respectively jsdnet instance fitted without training augmentation displayed even bigger landmark localization deviation 10.6 28.6 28.9 voxel unit dataset triplet next result gleaned ablation study conducted evaluated additional experiment three-fold mono-parametric variation foundational baseunet architecture training protocol evaluated performance summary plot ablation experiment provided supplementary material sect s.1 generally training augmentation protocol unet network depth proved largest modifier generalization performance augmenting training data effect segmentation performance small detrimental effect localization ability validation data crucial generalizing well test datasets example segmentation dice score dropped 0.05 best-performing augmentation protocol aug _\beta\ augmentation protocol similarly mean localization deviation increased approximately voxel unit protocol furthermore attention gating applied head heatmap scheduling deep supervision proved beneficial performance realm activation function mish gelu silu exhibited stronger performance carryover full test set average traditional rectifier variant relu prelu leaky relu loss wise joint cross-entropy-dice-loss exhibited largest segmentation performance drop-off validation test set average comparatively proved optimal localization performance gradient accumulation iter =15\ iteration proved optimal term average test set performance tested instance sgd could achieve competitive overall performance within gradient update step budget tested learning rate setting comparison adaptive optimizers notably comparison landmark localization affected negatively sgd optimizer segmentation performance weight regularization running average parameter setting adamw optimizer observed conflicting result parameter combination optimal one task non-optimal effect however small normalization layer wise instance norm exhibited better segmentation performance batch norm validation test dataset batch normalization yielded better mean test-set average performance enlarged standard deviation localization performance situation inverted batch normalization performance superseded instance normalization performance test set average produced final resulting gold-status network architecture via manual greedy collection task-optimal design choice using insight generated ablation study resulting architecture encompassed global attention gating instance normalization mish nonlinearity augmentation protocol aug _\beta\ heatmap scheduling protocol deep supervision gradient accumulation iteration experiment overall quantitative evaluation result segmentation criterion dsc iou jointly reported table validation hold-out clinical test open-source datasets corresponding landmark coordinate regression evaluation result stated table excellent segmentation performance validation dsc 0.97 test dsc 0.94 dataset observed indicated quantitative score automated segmentation revealed anatomically realistic small none erroneous volume apart main inner ear structure open source dataset dsc 0.94 segmentation dice score comparable test dataset observed dataset exhibited reduced segmentation performance dsc 0.89 segmentation map prediction final open-source dataset also slightly reduced overlap resulting overall dice score dsc 0.91 microct dataset landmark localization performance generally followed suit average localization deviation framework 3.3 voxel unit 5.2 voxel unit validation test set similar segmentation result localization deviation instance dataset lowest open-source test datasets average deviation 5.0 voxel unit localization performance difference prediction remaining test datasets comparison in-house validation datasets comparatively smaller automated network able locate landmark average deviation 5.4 voxel unit dataset 5.8 voxel unit dataset i.e comparable localization accuracy datasets set illustrative rendering automated machine prediction result multiple test dataset instance shown fig image show true positive prediction volume green well false negative yellow false positive red voxel classification selected actual instance instance dice overlap score shown instance prediction match global dataset average score provided table yielding representative dataset-wise visualization ground truth position landmark helicotrema oval round window marked blue sphere sphere posse radius voxel unit providing visual cue standard measure reported voxel unit deviation value shown table average localization deviation datasets well shown blue deviation sphere table evaluation result gold-status network segmentation subtask different datasets full size table table evaluation result landmark localization subtask different datasets full size table dataset selected instance shown fig visibly contained separate false-positive volume performance framework regard gauged hausdorff metric value provided latter part table basal hausdorff distance 6.4 voxel unit 9.7 voxel unit validation test set show dataset-aggregated worst case misclassified volume open-source dataset observed value five-fold enlarged compared test dataset averaged companion value increased similarly five-fold figure overview prediction result test datasets upper left upper right lower left lower right machine prediction true positive volume depicted green compared ground truth i.e manual label false positive oversegmented voxels colored red undersegmented false negative classified voxels colored yellow full size image apart qualitative consideration efficiency benefit proposed solution stated study time manually accurately label relevant anatomical structure landmark new dataset instance approximately using described threshold-based masking manual painting method basic gpu-accelerated pytorch implementation average prediction time amounted typical input volume tta strategy ten augmenting transformation voxels overlap per sub-volume chunk time included loading dataset instance disk addition evaluation clinically regular instance dataset single otosclerotic dataset instance also processed described pipeline algorithm significantly lowered segmentation performance well landmark localization performance compared regular instance dataset observed ground truth segmentation map depicted fig two exemplary slice image tta protocol additionally computed aggregation standard deviation prediction supersamples comparison mean volume-average standard deviation per voxel instance corresponding parameter singular otosclerotic instance showed approximately 15-fold increase 6.4 10^ 9.81 10^ figure two differing slice view raw data overlaid ground truth annotation clinical dataset instance otosclerotic lesion corresponding plot average prediction standard deviation per voxel coordinate prediction dice score coordinate panel show distinct clustering non-pathological datset instance upper left corner high segmentation performance low standard deviation apart otosclerotic instance lower right corner low segmentation performance higher standard deviation tta augmentation standard deviation computation scheme might leveraged perform preliminary uncertainty quantification full size image discussion motivated resource constraint drawback manual analysis volumetric radiological temporal bone image automated pipeline-based framework jointly produce inner ear segmentation map localize three important anatomic landmark without requirement user interaction developed loading dataset instance user interaction necessary production scan analysis consisted prediction inner ear anatomy union cochlea vestibulum semicircular canal coordinate-wise localization three important anatomical landmark helicotrema oval round window primary objective machine learning optimize function limited training data generalizes well genuinely new unseen data instance described result provide interesting evaluation due fact trained frozen cadaveric specimen tested scan clinical practice see table obtained similar scanning parameter generalization ability practical real world clinical setting could demonstrated markedly high dsc score good localization accuracy held-out test data compared catalogue related work presented table specified framework able achieve superior segmentation performance dice score 0.97 0.94 core validation test datasets respectively performance gain also translated total test dataset average dice value 0.92 outperformed inner ear segmentation score related work example neve reported dice score 0.91 ah-net hussain accomplished score 0.900 autocasunet proposed framework able predict segmentation map dice score 0.91 dataset hold-out test setting enabling direct commensurability hussain al. also employed dataset core training validation data note however performance result commensurable aspect single authoritative benchmark dataset like medical segmentation decathlon dataset created inner ear temporal bone segmentation strategy would help disentangle performance influence genuinely attributable pipeline algorithm possible candidate therefore e.g voxel size labeling quality basal imaging setup localization task method able provide highly accurate localization three anatomical landmark see table voxel unit deviation sphere fig general feature size membraneous structure case oval round window interestingly dataset-wise evaluation localization distance error helicotrema generally larger compared oval round window distance error judged surprising result since helicotrema expected generally located bone-cavern interface high radiodensity gradient well captured segmentation surmise ground truth label differing instance subject considerable variation due difficulty determining true extremal point cochlea respect modiolar axis apart numerical quantification congruence score examined spatial error distribution provide insight ability methodology produce segmentation clinically deployable e.g envisioned visualization inner ear anatomy intervention planning reviewing core dataset clinical origin model mostly exhibited benign error behaviour meaning voxel misclassifications generally restricted transition osseous structure cavity see false positive false negative error voxels fig i.e thin interface volume large radiodensity gradient addition transition surface inherently difficult segment region like ambiguous soft-tissue membrane e.g oval round window nerve tissue e.g cochlear nerve another focal region increased misclassifications design choice employ postprocessing algorithm like largest region extraction probabilistic active shape modeling open possibility dissociated erroneous false positive misclassifications advantage design choice allows gauge raw performance learned algorithm since metric computed direct model output actual clinical application setting misclassified region apart main structure would require manual cleaning real-world use postprocessing like registration shape model largest region extraction beneficial pure segmentation quality standpoint observation misclassification located close ground truth underlined small average hausdorff distance see table value due fact metric quantifies average distance-based error machine prediction contrast volumetric congruence error quantified dice intersection-over-union volumetric similarity maximum hausdorff distance outlier-sensitive metric provides insight indicating existence separate misclassification compartment pronounced open-source datasets especially dataset assume contrast alteration induced various preservation method open source datasets procedure invasive one drilling multiple immersion resin solution effect data domain shift fully corrected even specialized processing summarizingly anticipate described benign error behaviour automated framework permit manageable integration fully automated cochlear duct length measurement developed segmentation localization framework inclusion test-time augmentation proved beneficial two differing aspect in-house datasets external open-source datasets respectively namely performance improvement open-source datasets accompanied complementary use tta-produced prediction supersamples in-house datasets create information uncertainty open-source datasets profit tta improved segmentation localization performance voxel-wise average prediction map tta network receives multiple rotational point view data originally oriented according standard operating clinical procedure hand tta could employed compute prediction uncertainty map multiple tta prediction supersamples standard deviation map exemplary depiction supplementary material s.2 may interpreted tentative prediction uncertainty estimation thusly provide user additional spatial confidence map observable congruence true positive true negative misclassification map prediction supersamples standard deviation map encourages approach tta-based uncertainty estimation facilitate applicability data-driven automated model helping identify overconfident incorrect prediction thus inclusion tta proved valuable element proposed framework possible application mission-critical scenario like preoperative planning cochlear implantation dataset instance contained otosclerotic lesion see contrast alteration label surroundings panel fig processed markedly lower segmentation localization performance automated framework larger misclassified volume caused contrast bone texture alteration present otic capsule pathologically remodeled otosclerosis thusly dataset instance present interesting anatomy-wise out-of-distribution performance gauge default forward-styled reasoning conclude diminished performance devised system trained described dataset applicable pathological input data however noted global volume-mean standard deviation per voxel substantially larger corresponding value non-pathological dataset instance fig dice score plotted prediction sample standard deviation clear separation evident hence speculate line inverted reasoning tta conjunction computation prediction standard deviation may possible use case out-of-distribution subsequently pathology detection mechanism voxel-wise mean standard deviation prediction computed without ground truth information raised value parameter may serve signal pathology investigation dataset instance featuring otosclerotic lesion could elucidate tentative connection 10-fold reduction analysis generation time comparison manual labeling impressive demonstration efficiency gain automated pipeline using task automation large scale multi-centered clinical study within reach may unattainable earlier lengthy manual methodology rapid generation patient-specific anatomic analysis also facilitates integration routine pre- intra-operative planning cochlear implant surgeon prediction time pipeline potentially minimized even providing optimized implementation example parallelization strategy preprocessing tta augmentation computation could prove feasible basal model inference time could reduced network pruning sparsification u-network applying weight quantization despite consideration presented performance metric efficiency gain clinical applicability still requires much in-depth validation work presented well-performing formulation joint segmentation landmark localization segmentation wise neve presented blinded review seven expert participant automated segmentation dice overlap score 0.91 rated highly accuracy since described pipeline surpasses segmentation performance result encouraging respect real world application concerning localization performance potential future use automated cochlear duct length cdl measurement note clinical application translation error propagation helicotrema round window localization final cdl relevant proposed framework able locate helicotrema round window clinical data mean deviation 6.4 4.5 voxel unit using cursory worst-case error analysis physical unit via voxel edge length worst-case error cdl measurement initialized landmark position amount value 1.08 example taeger showed cdl measurement deviation magnitude clinically relevant respect tonotopic frequency layout cochlea limitation due pervasive large need data deep learning method one impediment presented work relatively small number dataset instance utilized train test learned model testing clinical datasets suffer inner ear contrast alteration rotational difference observed preserved specimen open-source dataset thus requiring enhanced processing desirable bottleneck labeling in-house instance time-intensive process manual segmentation high segmentation localization performance framework may provide potential bootstrap larger collective datasets via prediction newly recorded volume get quickly reviewed refined reintroduced training dataset evaluation focused healthy subject explicitly treated pathological dataset instance out-of-distribution special care given collection pathologically altered datasets e.g otosclerosis fibrosis better understand out-of-distribution performance model dataset instance data sourcing case often even difficult non-pathological case may necessitate specialized data augmentation transfer learning strategy another considerable limitation absence fully authoritative ground truth data concerning segmentation map landmark position ground truth data datasets manually generated researcher trained supervised qualified clinical researcher otological surgeon subsequently intra-observer variability may exist datasets effect systematic labeling difference external open source dataset catalogue incorrect inconsistent segmentation mask considered absolute ground truth could thusly lead low dsc evaluation automated prediction possible scenario lower dsc would paradoxically indicate better automated prediction comparison manual annotation viable remedy would general input data improvement strategy using multitude anatomical expert jointly segment dataset instance gold standard consensus ground truth could distilled endeavour could potentially culminate development earlier mentioned authoritative benchmark dataset temporal bone segmentation landmark detection improving commensurability reproducibility various automated methodology prospect directly addressing mentioned data limitation worthwhile line work could scope-maximized evaluation study using temporal bone data would ideally entail datasets exhibiting realistic inner ear fluid space contrast volume orientation according clinical standard operating procedure using clinical data generated related work summarized table would interesting attainable evaluation study looking beyond described framework immediate performance consideration identify integrated analysis possibly complex anatomical parameter like cochlear duct length measurement cochlear modiolar axis detection registration within unified fully automated pipeline work target inclusion transfer learning study learning multi-modal imaging data including mri strike another salient line research future investigation enhancement methodology focus beneficial objective conclusion presented automated u-net framework data processing pipeline provided highly accurate segmentation inner ear structure combined good localization anatomical landmark helicotrema oval round window via heatmap regression practical applicability demonstrated via evaluation representative flat-panel volume data secondary reconstruction sourced actual clinical practice robust generalization skill shown three-element catalogue processed open source datasets designed model incorporated state-of-the-art deep learning technique implemented modular architecture using open source software library conducted mono-parametric ablation study illuminated performance impact architectural design choice training protocol hyperparameters allowed inference task-optimal parameter bi-headed u-net based core architecture end-to-end trainable without postprocessing enabling insight raw performance deep learning based segmentation landmark localization model ablation study bi-headed solution single-task formulation yielded clear multi-task performance benefit landmark localization segmentation performance equal automated model skill small loss generalization skill open-source data visible comparison related work able show superior joint segmentation localization ability encouraging evaluation result automated methodology yielded good generalization ability proposed proof-of-concept framework may possess—with significant improvement validation—the potential utilized clinical practice research immediate tangible application segmentation localization result comprise e.g preoperative planning intraoperative visualization large scale exploratory clinical research