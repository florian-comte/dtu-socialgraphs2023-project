introduction phishing technique cyber-criminal also known attacker clone website interface sends compelling message naive user email social medium chat open link message similar cloned interface opened user open link username password entered interface sent attacker exploited number phishing attack increasing according anti-phishing working group apwg phishing attack reported december highest monthly total apwg history phishing attack become serious threat need detected fly user get trapped year phishing attack matured using advanced phishing method web development technology become prone detection continuously evolving adapting evade current intrusion detection system intrusion prevention system generally phishing detection system classified two group i.e. rule-based ml-based rule-based phishing detection system blacklist malicious domain url manual effort required update new domain website system detect first-time attack zero-day attack also detect false positive incident sample normal system predicted malicious due human error labeling ml-based technique hand utilize historical data phishing page find pattern webpage content url web page ml-based method state-of-the-art phishing detection perform better rule-based detection system although ml-based method effective phishing detection system trained historical datasets fail detect sophisticated crafted phishing sample future due change feature distribution regular model retraining upcoming sample mitigates issue lead performance deterioration older sample detection tackle problem adopted cl-based algorithm maintain performance old new phishing attack detection last layer existing model retrained adapt new dataset model retrained adapt new data maintaining performance previous data shall discus detail method section methodology specific contribution study identify performance deterioration time traditional system phishing detection due excessive change feature distribution evolution web development technology propose cl-based phishing detection framework cope performance drop issue traditional model show approach improves learning performance limited training data requires retraining time rest paper organized follows provide related work section related work define research tool method section research tool method present methodology section methodology describe experiment result section performance comparison continual learning perform analysis section discussion finally conclude section conclusion future work related work phishing attack grow researcher putting effort providing reliable resilient solution automatically detecting phishing attack current ml-based phishing detection technique classified based feature used detection i.e. url feature content feature visual hybrid feature based detection detection technique discussed following section url feature based detection phishing detection using url effective offline training data study identified extracted hand-crafted feature url used feature alone combination content-based feature train model used support vector machine svm decision tree random forest model achieved accuracy recent study used deep learning-based method convolutional neural network cnn generative adversarial network gan url generated gan model solve data bias problem caused imbalance phishing datasets method achieved accuracy 95.6 wei proposed novel multi-spatial character-level model applied url using cnn fast accurate phishing detection patil reviewed url-based technique developed model overcome issue bias found previous work sherazi observed url best way detect phishing website proposed system domain name phishers control url change domain name developed faster unbiased system seven feature accuracy 97-99.7 tian highlight url-based detection method limitation particular domain name internationalized domain name allow attacker register domain similar famous domain using different character local language look similar legitimate domain furthermore url-based detection method lack detection advanced phishing technique using webpage content feature content based detection recent study used particular keywords webpage content discriminative feature robust phishing detection researcher employed algorithm tackle phishing detection problem achieved promising result advancement feature extraction detection model made study instance content-based feature extracted nlp reinforcement learning ensemble bagging training others many recent study used nlp-based feature extraction email manuscript trained model study proved success nlp-based feature detecting phishing email accuracy 98.2 smadi used reinforcement learning extracted four feature including embedded url html content email header email manuscript feature dataset approximately ten thousand data sample obtained 98.6 detection accuracy ubing used extract feature nine best content-based feature selected used train ensemble classifier system used majority voting method avoid model overfitting achieved accuracy zamir introduced new approach using multiple algorithm used ensemble neural network bagging achieve 97.4 accuracy detecting phishing web page study show ensemble technique among best detection strategy phishing web page detection niakanlahiji crafted many feature html content code complexity certificate feature devise target-independent detection method result achieved 95.4 accuracy zheng feature embedding detect phishing page also combine character-level information word-level information embedding feature result quite promising accuracy 98.30 true positive rate 99.18 true negative rate 94.34 liu designed multistage model applies initial filter detect phishing page number page view etc moreover proposed framework called case extensive feature extraction proposed multistage model case framework could achieve recall 0.9436 precision 0.9892 measure 0.9659 tan proposed new technique detection based graph theory used hyperlink page create graph feature represent deeper information feature experimental result showed accuracy 97.8 presented three type web phishing detection feature original interaction feature used deep belief network model achieved around true positive rate 0.6 false positive rate study used feature selection method improve accuracy instance chiew proposed new feature selection technique called hybrid ensemble feature selection consists two phase produce set baseline feature hybrid ensemble feature selection show best result used classifier achieves accuracy 94.6 20.8 feature used originally content-based technique rely content-specific feature technique robust extent also capture advanced evasion technique attacker use code obfuscation hand-crafted content-based feature helpful anymore phishing page detection furthermore technique require time process website content runtime need fast efficient method use deep vector embedding representation content resilient phishing web page detection visual hybrid feature based detection recent work used visual-based feature combined text feature improve phishing detection visual feature computed cosine similarity phishing page corresponding legitimate page rao used hybrid detection approach employ ml-based visual similarity-based detection detection mechanism requires vast image database compare phishing web page achieved 99.55 accuracy using html content-based feature url-based feature third-party feature however third-party feature online feature costly slow making approach impractical real-time deployment tial identified squatting domain class obfuscation technique essential feature robust learning extracted keywords visual screenshots page using optical character recognizer ocr also converted code vector using feature embeddings develop robust detection model achieved accuracy better naive bayes knn hand chiew used website logo detect phishing website applying two-stage method first stage extracting logo website using google image search find domain name corresponding extracted logo method compare domain query website domain retrieved classifies according url visual hybrid features-based technique pretty robust phishing detection however technique pose new challenge computational complexity model time extract feature runtime moreover existing literature phishing detection yet consider performance deterioration ml-based system time thus need efficient robust adaptive model solves existing work problem best knowledge prior research exclusively employed traditional vnns limitation addressing novel phishing technique trained fine-tuned newly acquired data also method may face performance drop deployed scenario phishing attack changing rapidly therefore life-long phishing detector useful deploy scenario retaining previous data costly phishing attack evolving continuously also deployed detect zero-day attack also algorithm generally used computer vision retrain existing model adopt new task without forgetting knowledge prior task applied algorithm like learning without forgetting lwf elastic weight consolidation ewc reduce systematic performance drop phishing detection model lwf technique enables model learn new task without forgetting previously learned knowledge ewc method selectively reinforces important weight neural network prevent catastrophic forgetting research tool method section briefly describe different embedding method technique embedding technique range nlp embedding model different size capability specific task requirement computation resource availability influence selection embedding model well-known embedding model word2vec glove fasttext elmo bert briefly described follows mikolov proposed model called word2vec trained wikipedia page effectively learns semantic relationship word two variant continuous bag word cbow skip-gram cbow model predicts target word based context skip-gram model predicts context word based target word word2vec embeddings size range 100–300 pennington introduced global vector word representation glove glove another widely used word embedding technique trained large co-occurrence matrix word generate embeddings capture syntactic semantic relationship word glove embeddings size known effectiveness capturing global word co-occurrence pattern however may perform well out-of-vocabulary word bojanowski presented method called fasttext based word2vec model also incorporates subword information break word smaller character-based n-grams learns embeddings n-gram complete word allows fasttext capture morphological information handle rare out-of-vocabulary word better embedding technique fasttext embeddings typical size range 100–300 devlin designed bidirectional encoder representation transformer bert state-of-the-art word embedding technique transformer model generate contextualized word embeddings trained large amount text data unsupervised manner generate embeddings capture meaning word context bert embeddings typically size range 768–1024 known effectiveness learning complex nlp task question-answering etc cer suggested method called embeddings language model elmo contextualized word embedding technique generates embeddings based entire sentence context bidirectional lstm neural network generate embeddings capture meaning word context elmo embedding vector dimensional known effectiveness capturing complex relationship word handling polysemy however computationally expensive train require large amount training data studying embedding technique literature found fasttext suitable good balance embedding size vector quality making popular choice many nlp task also fasttext trained common crawl dataset consisting web page related problem word2vec glove small used non-complex task elmo bert complex model used complex task technique technique involves taking knowledge learned one task applying new problem tl-based technique fine-tuning iii domain adaptation progressive neural network pnns multi-task learning technique briefly explained follows fine-tuning involves taking pre-trained model adjusting perform new task pre-trained model usually trained large dataset fine-tuning process involves training model smaller task-specific dataset fine-tuning weight pre-trained model adjusted better fit new dataset retaining learned feature original dataset process significantly reduce training time computational resource required train new model scratch lange proposed method called adapts new information time main objective develop model learn retain new information without forgetting previously learned knowledge critical real-world application data constantly changing evolving various algorithm regularization lwf ewc etc. used improve model continuously time multi-task learning multiple task learned parallel generalize model requires data availability task present training life-long learning scenario model adapted new task time using new available data rusu introduced pnns consist series neural network specialized performing specific task network trained specific task used independently make prediction new task introduced new network added series previously learned network frozen kept unchanged pnns require careful design network architecture training procedure ensure added column neuron interfere existing knowledge pnn challenging implement optimize computationally expensive hence infeasible life-long learning model previously trained source domain adapted target domain output source target domain input data distribution entirely different requiring data availability domain increase training complexity high storage computation cost may face catastrophic forgetting number retraining iteration increase hence infeasible life long learning studying different technique selected fine-tuning cl-based algorithm require retraining time low computation cost methodology section cover detailed discussion methodology identify mitigate performance drop analyzed dataset feature using low-dimensional principal component analysis pca embedding sample visualize difference distribution transformed feature 1-d 2-d pca shown fig respectively figure show feature distribution 1-d pca trend data sample taken consecutive year x-axis represents 1-d feature value y-axis represents frequency sample depicts change distribution feature one year gap similarly analyzed 2-d pca feature fig figure show scatter graph normal sample figure show distribution phishing sample similarly fig show distribution phishing normal sample collected consecutive year result show feature distribution shift year inevitable implies one model trained historical data perform consistently well subsequent year data therefore solution required cope changing data distribution figure distribution shift one-dimensional pca representation three year data full size image figure distribution shift two-dimensional pca representation three year data full size image dataset best knowledge standard dataset phishing sample therefore collected phishing sample reported community platform like virustotal phishtank dataset contains extensive sample training model across subsequent year understand performance drop time dataset contains two type web page i.e. phishing benign detail follows phishing sample set contains around 90k html web page sample submitted famous community website phishtank virustotal malicious page dataset contains sample active consecutive three year i.e. 2018–2020 marked phishing sample ten reputed antivirus vendor virustotal benign sample dataset contains around 80k html web page submitted virustotal three year marked phishing malicious antivirus vendor use low threshold antivirus detection vendor known highly sensitive sometimes detect unknown non-famous benign website phishing due high sensitivity phishing detection vendor web page three fewer phishing detection considered benign sample table show number sample per year table dataset distribution full size table performed training testing experiment six datasets show performance drop divided three consecutive year datasets two equal size datasets named 2018a 2018b 2019a 2019b 2020a 2020b data pre-processing feature extraction first pre-processing step excluded anomalous website sample page little html content next step analyzed dataset feature model training many hand-crafted feature proposed recent study helpful phishing detection feature must exhaustive detect advanced phishing attack website evolve continuously employ deep feature representation full html content code text address issue using vector embedding vector embedding contains compressed important information easily use learn non-linear function used powerful embedding model called fasttext trained wikipedia page fasttext produce embedding dimension size dimension size good balance vector quality model complexity experiment result evaluate performance proposed cl-based phishing attack detection system performed experiment vnn model experiment used two deep-learning technique lwf technique enables model learn new task without forgetting previously learned knowledge ewc method selectively reinforces necessary weight prevent catastrophic forgetting section cover detail experiment result model trained six vnn model using dataset tested trained model datasets 2018a 2020b compare accuracy training validation testing ratio percent respectively table mention hyper-parameters used experiment moreover configuration used experiment vnn experiment table hyper-parameters used experiment full size table experiment vnn figure vnn architecture full size image vnn consists input layer one hidden layer output layer layer comprises multiple neuron neuron performs weighted sum input add bias term pass result non-linear activation function relu tanh sigmoid result passed subsequent layer output layer error actual predicted output computed using loss function cross-entropy loss loss minimized using optimizers like sgd adam backpropagated network computing gradient layer figure show architecture vnn model used experiment input layer consists 300-dimensional feature vector four hidden layer neuron respectively sigmoid activation dropout used hidden layer used binary cross-entropy loss function adam optimizer binary classification task phishing vs. benign figure a–f show comparison six experiment vnn experiment involves one training set time tested datasets experiment show vnn give high accuracy dataset used training lower accuracy datasets clear result vnn performs well current task gradually drop accuracy previous task i.e. figure show vnn trained 2020b achieves 2020b model achieves previous datasets experiment result achieved current dataset used train model best term accuracy use benchmark compare accuracy figure vnn achieves high testing accuracy current dataset able perform remaining datasets full size image experiment well-known technique deep learning fine-tune pre-trained model perfectly trained relevant problem new task retrain last layer network freeze initial layer use generic feature learned layer depicted fig training regime reduces model convergence time save computation resource help learn small datasets however model work well new data forgets knowledge old data experiment fine-tuned last two layer vnn architecture shown fig pre-trained vnn model trained first chunk 2018a tested datasets model fine-tuned second 2018b dataset fine-tuned 2020b observed retrain model decrease performance previous datasets inevitable figure block diagram red color show fine-tuned layer full size image figure show accuracy result using make improvement accuracy model retrained following task however practical solution problem also deteriorates performance old task experiment fig a–f represents several retraining applied 2018a model like experiment two fig represents 2018a model retrained new dataset 2018b achieves accuracy 2018b reduces accuracy old task 2018a similarly fig show retraining recently fine-tuned model 2019a achieves accuracy reduces accuracy old datasets 2018a 2018b finally fig show accuracy 2018a drop retrained five subsequent task achieving high accuracy 2020b inherent drawback forgets previous task moving forward figure gradually decrease accuracy previous datasets number retraining task increase full size image experiment learning without forgetting training paradigm allows model continuously adapt learn new task without losing knowledge old one even without access old data unlike model work new data technique designed work old new data using model trained old task apply incrementally make work dataset belonging subsequent year used two state-of-the-art technique known learning without forgetting lwf discussed elastic weight consolidation ewc discussed following subsection figure learning without forgetting architecture full size image lwf technique retrain model new task impose penalty loss function enforces maintain performance old task task set input feature corresponding target used retraining model figure show two part lwf model shared weight _s\ task-specific weight _o\ shared weight contain combined knowledge task task-specific weight added runtime number task increased future lwf algorithm doe require data belonging old task based intuition stabilizing output neuron belonging old task new data model doe change weight neuron essential old task algorithm explains step-by-step process retrain model new task base vnn model i.e. vnn first trained 20218a dataset task-specific layer _n\ added new task 2018b retraining record output _o\ old task enhanced model train model adjust parameter work well old new task using data belonging new task performs two main computation retraining enforces model maintain old task loss old constant make sure model doe forget current state old task reduces loss i.e. new new task head detection node added decision layer randomly initialized weight _n\ network learns new task minimizes loss new task using regularization stochastic gradient descent new task head added observe performance decrease base model shown fig task head used final decision-making using majority voting weightage-based decision latest task head used final decision experiment trained vnn model 2018a retrained model subsequent datasets using new task head observed lwf sustains detection performance compared used prediction latest task head compute accuracy new task figure a–f show result lwf observed lwf accuracy increase datasets maintaining performance previous datasets similar six experiment represents number retraining attempt happened 2018a model i.e. experiment fig represents result 2018a model retraining two new datasets figure show lwf achieves 2018a achieves last task retraining five time show method quite effective coping forgetting problem old task learning new task figure lwf sustains test accuracy current previous datasets retraining full size image experiment elastic weight consolidation elastic weight consolidation ewc another technique intuitive effective learning multiple task ewc find joint feature space old new task adding penalty loss function vnn figure show two task find common feature space task training penalty enforced training task encourages model change weight used importance task consequently model learns common feature space representing task ewc computes weight importance task computing fischer importance matrix training task penalty loss function fischer matrix shown term loss task overall loss penalized matrix overall loss decrease task loss penalty learn common feature space task observed difficult maintain high performance old task number task increase due weight saturation figure elastic weight consolidation conceptual diagram full size image aligned aligned figure a–f show ewc method result show ewc also learns well current dataset preserve knowledge previous training thus maintaining overall better performance accuracy task example fig show ewc achieves accuracy 2018a dataset achieves 2020b dataset quite close lwf result figure ewc also sustains test accuracy old task achieves slightly test accuracy current task lwf full size image four experiment observe vnn performs well current task give performance improvement cl-based method give promising result task shown fig performance comparison continual learning section describes overall comparison technique vnn tl-based method technique observed resilient reliable achieving lifelong high-performance detection phishing attack accuracy comparison figure show performance comparison best testing accuracy achieved six datasets four technique vnn trained separately dataset model retrained dataset see vnn give around accuracy significant performance loss loses performance every time retraining task increase technique outperformed compared approach accuracy close vnn accuracy considered benchmark accuracy thus cl-based algorithm give promising result real-time model retraining figure comparison best accuracy benchmark accuracy vnn achieved historical datasets full size image confusion matrix compare confusion matrix best performance experiment evaluate vnn technique confusion matrix four value first row represents true positive false negative first second column respectively second row represents false positive true negative model evaluation value higher show well model identify malicious sample figure show confusion matrix vnn technique experiment six datasets show vnn achieves score highest best result without retraining time lwf give best result datasets retraining also seen fig vnn high true positive rate cl-based method lwf ewc computational vs. accuracy trade-off therefore cl-based method achieved good accuracy time even little retraining vnn requires training scratch dataset score represents phishing page model miss-classified want number minimum possible system analysis show false-negative score overall lowest ewc method evident fig figure confusion matrix comparison best experiment vnn cl-based method show method close vnn retraining full size image discussion study investigates issue performance drop time model retraining trained model new data conduct experiment consider two cl-based method i.e. ewc lwf method relatively low computation memory cost require training data retraining time method e.g. pnns etc however cl-based method several restriction e.g. size learnable parameter increase new task added learning catastrophic forgetting may happen several training attempt adding new data iii data bias model becomes biased specific task seen experiment subsequently trained model six datasets achieved satisfactory result comparable vnn even little retraining new data however observed slight decrease true positive rate cl-based method model trained new data may forget previously learned data catastrophic forgetting therefore reduced performance vnn hence computational vs. accuracy trade-off best knowledge previous study used one independent datasets train evaluate ml-based method phishing detection use datasets used previous study contain multiple year data need multiple year data prove idea phishing detection regard collected datasets three consecutive year present result existing ml-based phishing detection method like vanilla neural network compared show life-long phishing detection performance conclusion future work study identified performance drop traditional model time study performance drop issue conducted several experiment three different setting vnn model without retraining vnn model vnn model experiment result show vnn model high detection accuracy trained separate model every time new datasets different distribution requires training resource extensive training data training time want hold previous knowledge phishing tactic want adapt new attack reason experimented tl-based model deteriorated accuracy time retrained new data sample algorithm negligible effect changing data distribution due efficient retraining mechanism adapts new knowledge quickly maintaining previously learned knowledge therefore cl-based algorithm practically used first-stage phishing attack detection mechanism real-time environment promising long-term result cl-based algorithm practical application reducing false positive improving efficiency forming part overall phishing defense strategy future investigation performed reduce catastrophic forgetting new phishing technique introduced new task added eventually increase model complexity requires much retraining hyperparameter optimization finding best possible set parameter model size also considered future work also aim explore various embedding model like bert etc. extract powerful feature phishing detection consider investigating continual learning technique e.g. replay-based method task-specific learning etc. phishing detection future work finally also consider adversarial training cl-based model future