introduction considering prediction extrapolation physical quantity unknown domain reliance single imperfect theoretical model misleading improve quality extrapolated prediction fact advisable use several different model mix result way collective wisdom several model could maximized providing best prediction rooted current experimental information carry mixing statistical machine learning method ability capture local feature tool choice specifically bayesian model averaging bma used combine multiple model produce reliable prediction since natural bayesian framework account uncertainty model absence another established methodology application bma scenario several model compete describe phenomenon relatively widespread diverse field weather forecasting political science transportation nuclear physic important remember however bma relies assumption one proposed model true model i.e. model perfectly describes physical reality clearly inappropriate dealing complex system approximate modeling practice often happens none competing state-of-the-art model dominated others sense model doe something better others setup model viewed exclusive complementary bma seems theoretically ill-grounded addition standard implementation bma weight global i.e. constant input domain thus unable catch local model preference besides bma exist method combine result different model fact combining model subject much research led development topical ensemble learning method bagging boosting method remain spirit purpose close bma typically fix inadequacy mentioned goal identify best performing model given set model see ref review additional approach work develop apply local bayesian model mixing lbmm extension bayesian stacking managing competing model bayesian stacking framework one assumes true model linear combination model instead one model extrapolation thus obtained via direct mixture model compared mixture posterior distribution standard bma unlike bma weight reflect fit statistical model data independently set available model except normalization weight based model mixing stacking reflect model contribution final prediction lbmm used study make use dirichlet distribution infer stacking weight hierarchically depend model input space thus highlight local fidelity theoretical model additionally lbmm framework well capture uncertainty individual model mixing weight proposed hierarchical structure first present general lbmm framework followed pedagogical case global mixture model corresponds classical bayesian stacking subsequently let model weight vary across model input space consider several hierarchical bayesian model based dirichlet distribution example apply new method predicting nuclear mass binding energy basic property atomic nucleus since consider bma point comparison lbmm methodology briefly review general predicative framework bma method section binding energy determines nuclear stability well nuclear reaction decay quantifying nuclear binding important many nuclear structure reaction question understanding origin element universe astrophysical process responsible nucleosynthesis star often take place far valley beta stability experimental mass known case missing nuclear information must provided extrapolation accurate value nuclear mass uncertainty beyond range available experimental data also used scientific field atomic high-energy physic well many practical application order improve quality model-based prediction mass rare isotope far stability approach used utilize experimental theoretical information broad range tool used mine unknown nuclear mass including gaussian process gps neural network frequency-domain bootstrap kernel ridge regression see recent review reference series paper bma methodology applied nuclear mass prediction work propose lbmm approach produce model-informed extrapolation nuclear mass overcome limit bma mentioned bayesian model mixing let consider experimental observation x_i physical process location x_i\in ^q\ i=1 governed true model let assume true model fully captured one proposed model f_k k=1 rather combination model natural consider statistical mixture model general form aligned x_i k=1 x_i f_k x_i aligned represents scale error mixture model iid 0,1 f_1 x_i f_p x_i theoretical value datum x_i provided theoretical model considered practice weight x_i x_i x_i must taken space inference possible done many way work highlight alternative model x_i tractable suggestive fully bayesian additionally one improve model accounting systematic error done adding model systematic error correction aligned x_i k=1 x_i f_k x_i +\delta x_i aligned global mixture first present simplest application bayesian model mixing bmm one assumes global weight gbmm i.e. weight constant input domain linear model let first model underlying physical process global linear mixture individual model aligned x_i k=1 f_k x_i +\delta x_i aligned using log-likelihood model written aligned -\frac 2\pi -\frac 2\sigma i=1 x_i k=1 f_k x_i +\delta x_i aligned x_1 x_n ensure weight support model weight bma may also justified assume simplex constraint _k\ge _k\omega _k=1\ case posterior distribution also satisfy simplex constraint first condition easily met using non-negative prior second challenging enforce prior nevertheless naive idea projecting unconstrained posterior appears relatively efficient case simple linear model projecting simplex constraint corresponds substituting aligned max _\ell _\ell aligned dirichlet model refinement global linear mixture step towards local mixture simplex constraint one suppose weight given hierarchically dirichlet distribution aligned _k^ aligned hyperprior hyperparameter reason additional modeling layer twofold first allows express uncertainty prior model weighing imposed note dirichlet distribution size parameter vector multivariate continuous distribution simplex k=1 average value _j\ k=1 _k\ looking shape distribution fig clear close model selection encourages true mixing hyperprior allows quantify uncertainty two regime secondly hierarchical model permit slight modification heterogeneity weight based value shall exploit shortly figure log density dirichlet distribution =3\ function _1\ _2\ _3=1-\omega _1-\omega _2\ parametrization dirichlet distribution close model selection one dominant weight high probability lead true mixing several large-probability weight left 0.3,0.3,0.3 right 1.3,1.3,1.3 full size image consequently choice prior joint posterior distribution given aligned aligned doe closed form general need approximated using mcmc prediction observation physical process new location obtained propagating posterior sample hierarchy described dirichlet weight encapsulate contribution model mixture make interpretation weight probabilistic sense carry different meaning bma weight measure fidelity individual model still weight compared play role final predictions—keeping mind information contained posterior distribution dirichlet weight richer point value produced bma local mixture let consider observation x_i follow general statistical model given key feature weight depend location without additional information function x_i shall estimated non-parametric estimator satisfying simplex constraint order account local dependency model weight satisfying simplex constraint propose hierarchical framework based dirichlet distribution specifically take every weight dirichlet random variable defined parameter correlation different value shall contained corresponding -correlations investigate two model generalized linear dirichlet model gld gaussian process dirichlet model gpd particular assume every location model weight follow dirichlet distribution parameter depending encode spatial relationship model weight input space since dirichlet distribution defined parameter value additionally apply exponential link function i.e. consider =\log regressed symmetrically thus purpose link function allow unconstrained modeling gld version local mixing framework represents parametrically aligned ^t_k aligned 1,1 p,1 parameter vector linear nature corresponds assumption correlation local weight relatively large spatial range finer version lbmm propose non-parametric gpd model defined gaussian process prior parametrized constant mean _k^ covariance c_k given quadratic exponential kernel additionally assume statistically independent note proposed hierarchical structure take account relationship different value via also correlation weight model given spatial location via dirichlet distribution would possible one choose model directly let say application nuclear mass extrapolation case study bayesian model mixing framework described consider separation energy atomic nucleus subject previous investigation particular goal compare following alternative raw model without statistical correction _f\ _f\ versus model corrected _j\ _f\ bma versus global bmm global bmm versus local bmm two-neutron separation energy fundamental property atomic nucleus defined energy required remove two neutron nucleus expressed difference nuclear mass input space represented number proton neutron consequently study q=2\ x_i z_i n_i y_i\ observed two-neutron separation energy x_i\ particularly interested even-even nucleus even number use recent measured value two-neutron separation energy nucleus ame2003 dataset training data 521\ bmm systematic correction bma calculation retain evidence dataset subset training data consisting nucleus proton-rich nucleus neutron-rich nucleus keep additional data tabulated ame2020 sample extrapolative testing dataset n=59 three domain depicted fig figure training black dot testing red circle evidence red dot datasets two-neutron separation energy even-even nucleus used study eight evidence nucleus also included testing dataset nucleus represented number proton neutron see text detail full size image prediction use largest domain two-neutron separation energy positive i.e. corresponding nucleus predicted exist line previous study consider seven theoretical model based nuclear density functional theory dft capable describing whole nuclear chart skm skp sly4 sv-min unedf0 unedf1 unedf2 dft data taken theoretical database set dft model augmented two well-fitted mass model frdm-2012 hfb-24 significantly parameter phenomenological dft model resulting better fit measured mass subsequent bayesian analysis use independent prior different statistical parameter keep consistent notation throughout model mixing variant general use normal prior unconstrained parameter gamma prior positive parameter uniform prior bounded parameter recall gamma distribution parametrized shape parameter rate parameter mean given variance given b^2 error scale parameter use gamma prior scale parameter rate parameter mean 0.5 mev standard deviation 0.22 mev case lbmm variant gld defined take independent normal prior distribution mean standard deviation element _p\ lbmm gpd used independent squared exponential kernel namely c_k -\frac 2\rho -\frac 2\rho characterized three hyperparameters chosen take length-scale parameter common nuclear model leave intensity parameter different model order ensure stability convergence result gpd weight follow frequency residual model case dirichlet distribution gpd local mixture take independent normal prior mean variance mean parameter _k^\infty\ independent gamma prior three scale parameter _k\ prior taken respective parameter corresponds slightly informative prior helped ensure convergence towards weight localized appropriate scale parameter _k^\infty\ determines long range weight model i.e. far training data note taking zero-mean i.e. setting _k^\infty would amount uniform weight far data come global gbmm+l mixture take independent uniform prior practice simplex constraint satisfied implicitly without need apply confirms individual model well conceived gbmm+d variant take half-normal prior standard deviation use systematic correction two-neutron separation energy residual i.e. difference theoretical measured two-neutron separation energy computed using bayesian gaussian process fixed prior training dataset agrees fig set additional nucleus follows use nucleus training whenever uncorrected model considered since would extrapolative perspective consequence omission negligible due overall size training set model weight trained value full training dataset exception bma used evidence set shown fig consists nucleus similar compute bma weight set representative nucleus computing evidence large dataset inevitably lead model selection happens due exponential multiplicative nature gaussian likelihood punishes large deviation favor good fit see detail weight turn applied obtain prediction proton-rich limit determined two-proton separation energy identified previous study table summarize result model variant discussed following paragraph table rms deviation mev individual model global bma gbmm local lbmm mixture full size table figure empirical coverage probability raw model without statistical correction together bma bmm variant empirical coverage probability calculated equal-tailed credibility interval reference line diagonal marked dashed line full size image uncorrected model versus corrected model first discus fidelity individual model end study root-mean-square rms deviation modeling variant discussed paper consider raw model prediction prediction including systematic correction seen individual model corrected variant generally outperform raw prediction see ref discussion exception hfb-24 carefully calibrated experimental mass case correction term _f\ doe lead lower rms deviation testing dataset want point mixing corrected model done caution empirical experience mixing gbmm lbmm previously corrected model lead overfitting one tends fit statistical model small leftover noise since residual corrected model training dataset practically zero clearly observed table rms deviation local global mixture slightly outperform combination corrected model testing dataset instance gbmm+d uncorrected model give 0.31 mev rms deviation testing dataset compared 0.46 mev corrected model also compared 0.35 mev rms deviation bma corrected model since providing accurate extrapolation main focus work following focus discussion primarily uncorrected model table global weight calculated training dataset different method bma evidence subset see fig obtained closed form computation well monte carlo laplace approximation two global mixture gbmm+l gbmm+d obtained whole training set full size table bma versus global mixture bma evidence integral calculated evidence dataset mean monte carlo laplace approximation closed form conjugate prior denote corresponding bma variant follows bma bma lap bma respectively see method section calculation bma weight table see model weight produced bma consistent across three evidence computation approach irrespective whether systematic correction applied averaging corrected model democratic compared raw model expected since gp-based correction fit training data closely irrespective theoretical model still bma testing rms uncorrected corrected model similar slight preference uncorrected model global mixture uncorrected model generally slightly outperforming bma training testing datasets see table fact expected given weight designed maximize predictive power model mixture indeed gbmm+l model bayesian counterpart frequentist linear regression different nuclear model prediction minimizes rms training set principle still hold despite uniform prior used gbmm+l model informative play regularizing role reduces overfitting favor mixing see dirichlet mixture model yield similar weight benefit weight natively located simplex comparison global weight already speaks favour ruling bma purpose combining imperfect model favor bayesian dirichlet model table also show posterior mean noise scale parameter comparison rule thumb statistical model conservative liberal uncertainty quantification would test rms statistical model high-fidelity close test rms comprehensive view reflects fully propagated prediction uncertainty gleaned fig show empirical coverage probability ecp curve fig corresponds proportion prediction testing dataset falling respective credible interval equal-tailed credible interval ecp curve closely follows diagonal actual fidelity credible interval corresponds nominal value thus see gbmm superior prediction performance better bma individual model global versus local mixture posterior mean lbmm+gpd weight shown fig plot lbmm+gld given supplementary information discussed earlier mixing model locally corrected systematic error highly susceptible overfitting therefore focus uncorrected model i.e without lbmm variant show dominance well-fitted hfb-24 mass model throughout nuclear landscape expected simplistic linear dependence weight gld variant insufficient fully capture complex local behaviour mass model learned flexible gld variant hfb-24 mass model dominates final lbmm+gpd result involve model primarily frdm2012 unedf0 skm weight distribution naturally depends choice model involved analysis suggests preselection diversified model used lbmm could also considered beforehand figure posterior mean local model weight lbmm+gpd variant across nuclear landscape full size image term rms deviation gpd variant doe better gld local mixture reflects ability gpd capture local performance mass model local mixture perform better global mixture training set bma training testing set however fall slightly behind global mixture testing set attribute difficult tuning statistical model sensitive variation parameter limited testing dataset term distribution across nuclear landscape fact markov chain monte carlo mcmc sampling bayesian posterior distribution numerically unsatisfactory conventional metropolis sampler due relative large number parameter lbmm order achieve satisfactory convergence recommend using sophisticated no-u-turn sampler tends perform well scenario moderately large parameter space see method section detail term fidelity lbmm variant clearly dominate global mixture bma individual model since ecp respective predictive credible interval closely match nominal value see fig discussion work propose implement bayesian dirichlet model mixing framework proposed method illustrated applying nuclear mass model ass local fidelity improve predictability raw theoretical model statistically-corrected version considered better understand interplay modeling bma bmm framework bayesian model mixing raw model result testing rms least good better bayesian model averaging irrespective model corrected clearly superior thus improving model prediction mixing rather mixing corrected model lead best performance term prediction accuracy since bmm trained sizable training set also robust choice prior bma prior sensitive evidence data weak bmm corrected model performed caution may lead overfitting case one likely achieves better improvement standard bma based well chosen evidence set lbmm+gpd variant achieved smallest training error training dataset 0.25 mev demonstrates lbmm well capture local presence individual model furthermore local mixture clearly surpass modeling strategy explored work term fidelity show proposed hierarchical dirichlet model lbmm effectively represents propagates uncertainty essential mass modeling unexplored domain result bma depend choice evidence dataset increasing density evidence data region interest e.g. application extrapolation one improve predictive power averaging procedure improvement performance bmm also achieved restricting training dataset region interest opposed training whole domain motivates introduction local bmm model distribution bma bmm weight also depend choice theoretical model table fig show mixing large set model result minimal contribution point existence class model similar local preference e.g. unedfn class indicates adding model preselection model orhogonalization bmm pipeline could lead improvement predictive performance fact context gbmm+l model well known collinearity proposed theoretical model source major instability method bma highlight let consider task predicting observation physical process new location using observation x_1 bma posterior predictive distribution aligned k=1 _k|\varvec aligned simply linear combination individual model posterior predictive distribution global model weight taken posterior probability _k|\varvec model _k\ true model given bayes theorem aligned _k|\varvec _\ell _\ell aligned aligned =\int aligned evidence integral model _k\ prior density model parameter noise scale _k\ systematic discrepancy data likelihood prior probability _k\ true model—assuming one model true handful statistical distribution evidence integral expressed closed form one scenario linear regression model conjugate prior statistical model _k\ constant discrepancy term belongs case model let consider prior aligned _k| _k| aligned conditionally theoretical model choice _k\ precision parameter inverse variance _k\ follows normal distribution mean variance _k\ let assign precision _k\ gamma prior shape parameter rate parameter evidence integral closed form solution aligned a_n b^a b_n^ a_n _n^ -\frac aligned a_n b_n d_i denoting d_i y_i y_k x_i d_i solution obtained simple tedious algebraic manipulation see ref detail stated main manuscript use parameter gamma prior scale rate parameter order match mean standard deviation ^2\ distributed according common gamma prior shape scale parameter result closed form bma obtained gamma prior precision inverse variance parameter shape scale parameter 0.252 0.030 evidence obtained explicitly estimate computed aligned p\big _k\big aligned sample prior distribution model parameter _k| alternatively discrepancy term considered constant evidence integral approximated closed form expression technique frequently used laplace method integral quadrature aligned p_l 2\pi _k|^ p\big _k\big _k|\mathscr _k\big aligned _k\ represent posterior mode -\varvec inverse hessian matrix second derivative _k|\mathscr gamma s^2 aligned ^2_k n-a+1 _k^2 3\sum x_i y_k x_i _k^4 x_i y_k x_i _k^3 s^2 _k^2 s^2 aligned computation evidence integral simplified scenario without constant discrepancy term statistical model contains single parameter _k\ leave detail simple exercise probability reader application summary modeling choice matter clarity guarantee reproducibility result presented section application nuclear mass exploration table list parameter choice prior modeling variant discussed note consider theoretical model without statistical correction x_i table summary statistical model parameter prior used section application nuclear mass exploration full size table mcmc computation mcmc approximate posterior distribution modeling variant discussed work obtained using hamiltonian monte carlo based no-u-turn sampler nut general obtained least 10^3\ sample posterior distribution discarded half burn-in conventional sampler metropolis-hastings algorithm would sufficient bma global mixture using nut essential achieve satisfactory convergence come lbmm illustrate provide selected mcmc traceplots lbmm+gpd variant nut approximation method fig respectively nut performance tends superior scenario moderately large parameter space clearly show convergence markov chain display poor mixing figure traceplots scale parameter mean parameter _k^\infty\ obtained via metropolis-hastings algorithm lbmm+gpd variant full size image figure similar method fig no-u-turn sampler full size image